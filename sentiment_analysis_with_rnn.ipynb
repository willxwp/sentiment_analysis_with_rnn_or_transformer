{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec90cf5",
   "metadata": {},
   "source": [
    "This project aims to demanstrate how to do sentiment analysis using RNN, specifically LSTM, on the IMDB dataset. The IMDB dataset is a collection of 50,000 movie reviews that are labeled as either positive or negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d975a",
   "metadata": {},
   "source": [
    "<b>Best practice</b><br>\n",
    "<br>\n",
    "LSTM is often considered the default choice for RNN models in practice due to its ability to effectively capture long-term dependencies in sequential data while mitigating the vanishing gradient problem. However, GRUs are also commonly used depending on the specific task and dataset characteristics. The choice between LSTM and GRU depends on the following factors:\n",
    "\n",
    "- <b>Model complexity</b>: LSTMs typically have more parameters than GRUs due to their additional gating mechanisms. If you have limited computational resources or are working with smaller datasets, GRUs may be more suitable due to their simpler architecture.</b>\n",
    "- <b>Training speed</b>: GRUs are generally faster to train than LSTMs. If training time is a concern, GRUs might be a better choice.\n",
    "- <b>Performance</b>: LSTMs tend to have better performance on tasks that require modeling long-term dependencies in sequential data. If your task involves capturing complex temporal patterns and you’re concerned about overfitting, LSTMs might be preferable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70fee6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Preprocessing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71173f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"movie_data.csv\"\n",
    "data: pd.DataFrame = pd.read_csv(data_path)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d00efe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f0847bed-c4cc-47be-8888-96e3cd582963",
       "rows": [
        [
         "8666",
         "A wonderful film ahead of its time,<br /><br />I think so, In the eighty's it was all about winning, Greed is Good ? Remember that one ? I have seen this film more that 20 times, To me this is a real desert island film, I keep watching because there is always something more to learn about these flawed characters that I just love, Jessica Tandy, and Hume Cronin, are simply wonderful,Also Beverly D'angelo, Beau Bridges come in at a close second, don't get me wrong there are many more great performance's in this film, and it is also the way it is written that made it for me, and I hope you, a film that you will want to see over and over, I think TV shows like \"Northen Exposure\", and now \"Earl\" owe a lot to this film. but remember it is not a Tom Cruise film.",
         "1"
        ],
        [
         "2880",
         "I saw this at the BendFilm Festival Friday amid an unsettled crowd of people, not helped by a poor decision by the planners of the event, who chose a totally inappropriate short film to precede the movie. And it really threw the audience when Modern Love came up after a light, whimsical short (name I forget). <br /><br />People!!! It was really silly to mix this short with Modern Love - which is a serious drama movie. A film film. <br /><br />So the audience gets the teaser which is a comedy and then...Modern Love. Hmmmm. Modern Love, despite my reservations (strange ending, a little too tangential)needed a short film that was commensurate with it's oddball strangeness, so my advice to the programmers for next year is to take more care planning the show.<br /><br />The folks watching Modern Love really just didn't know what had hit them, - they were led up the path and this is not their fault.<br /><br />Modern Love has some superb performances which play well against the tangential meanderings of the film - a film that its maker seems to have 'wondered out loud' rather than executed in the normal way a film is scripted and shot.<br /><br />Too bad the audience was misinformed. Wrong session placement, wrong short film, wrong approach by the well intentioned programmers, who, despite good efforts, need to see a lot more films and travel to some other festivals.",
         "1"
        ],
        [
         "29940",
         "When 'My Deja Vu, My Deja Vu' aired last season, I was pleased. Scrubs, I thought, is doing something clever and unique in regards to the clip-show concept. Instead of replaying footage, they're replaying jokes in a self-aware manner, and I really enjoyed it.<br /><br />I found it really unfortunate that I was wrong. One season later, they succumbed to that which almost all sitcoms inevitably do, the clip show...and it looked like it was put together by the work-experience kid. Dr Cox's shaved head shows just how lazy the editors were in putting it together, as it doesn't appear again until 'My Long Goodbye' some 4 episodes later. I can't imagine that a wig is too much effort when it comes to maintaining the continuity of what was once a well-constructed sitcom. Who knows why it was slotted there, it just seemed lazy and out of place, reminding me (largely) of episodes that have aired within the past year.<br /><br />Three second clips jammed together with background music is a DVD extra for a (very) rainy day, not an episode of prime-time television.",
         "0"
        ],
        [
         "45022",
         "I do not expect this film to be well understood by viewers out of Romania. This tells something certainly about the value, or maybe about the lack of universality of the film, but also tells something about how different history and even life of common people was in Romania compared to other countries, even in Eastern Europe.<br /><br />The film is an adaptation of a novel by Marin Preda, a controversial novelist who died during the Communist rule soon after the book was published. It tells the story of an intellectual, professor of philosophy whose life is crushed after he is imprisoned on false accusations at the end of the Stalinist era. Basically the first part of the film tells the story of his fight for survival in prison, the second describes his tentative to regain his life after being released. His release is actually only apparent, Romania of the 60s asks from him different types of compromises and crimes, but yet his fight for survival is as tough morally as in prison.<br /><br />The film is splendidly acted by some of the best Romanian actors. Stefan Iordache who has the lead role would be in another time and another place a mega-star, we can get here a good glimpse of his fabulous acting art. Although suffering from a hesitant story-telling and falling sometimes in non-essential details or character comics, the film is still an important landmark for the Romanian cinema, as well as for the process of recovering the moral and historic values in the Romanian society.",
         "1"
        ],
        [
         "46498",
         "First off let me say that this has to be on the top of my list of boring movies. Nothing, and I mean nothing in this movie is even remotely thrilling. Most of it is very confusing and as it progresses you just wish it would end!! Some people want a movie that makes them \"think\" through the entire thing, to which I say...\"More power to you\"!! I on the other hand just want to be entertained. Which brings me back to this stinker, entertainment it is not. This movie is stupid and a complete waste of time. Seems that most here agree also. Most of this didn't make any sense, and by the time you think you have one scene figured out another lame scene comes around and....well I guess you see where this is going. Avoid, this one sucks....bad!!",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8666</th>\n",
       "      <td>A wonderful film ahead of its time,&lt;br /&gt;&lt;br /...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>I saw this at the BendFilm Festival Friday ami...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29940</th>\n",
       "      <td>When 'My Deja Vu, My Deja Vu' aired last seaso...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45022</th>\n",
       "      <td>I do not expect this film to be well understoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46498</th>\n",
       "      <td>First off let me say that this has to be on th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "8666   A wonderful film ahead of its time,<br /><br /...          1\n",
       "2880   I saw this at the BendFilm Festival Friday ami...          1\n",
       "29940  When 'My Deja Vu, My Deja Vu' aired last seaso...          0\n",
       "45022  I do not expect this film to be well understoo...          1\n",
       "46498  First off let me say that this has to be on th...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, \n",
    "                                         random_state=42, stratify=data['sentiment'])\n",
    "\n",
    "# Split the train data into training and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=5000, \n",
    "                                         random_state=42, stratify=train_data['sentiment'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90c1942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 84179\n",
      "Labels: Counter({1: 15000, 0: 15000})\n"
     ]
    }
   ],
   "source": [
    "# Explore the vocabulary within the training set\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# define a tokenization function to remove special characters and split text into words\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<[^>]*>', '', text) \n",
    "    # find emoticons \n",
    "    emoticons: list[str] = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())  \n",
    "    # remove non-word characters and move emoticons to the end of the string  \n",
    "    text: str = re.sub(r'[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# create a Counter object to count the frequency of each word in the training set\n",
    "token_counts = Counter()\n",
    "for text in train_data['review']:\n",
    "    token_counts.update(tokenize(text))\n",
    "\n",
    "print(f\"Vocabulary size: {len(token_counts)}\")\n",
    "print(f\"Labels: {Counter(train_data['sentiment'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f90efe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 84181\n",
      "[11, 7, 35, 480]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We will feed the word tokens into an embedding layer, nn.Embedding. The embedding layer requires integer input because \n",
    "it’s specifically designed to handle discrete categorical data, such as word indices, and transform them into continuous representations \n",
    "that a neural network can work with. Therefore, we need to first encode each token into a unique integer / index:\"\"\"\n",
    "\n",
    "\n",
    "# Define a function to build the vocabulary mapping from tokens to indices and vice versa.\n",
    "def build_vocabulary(token_counts: Counter, \n",
    "                     min_freq: int=1, \n",
    "                     specials: list[str]=['<pad>', '<unk>']) -> tuple[dict[str, int], dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Build a vocabulary mapping from tokens to indices\n",
    "    \n",
    "    Args:\n",
    "        token_counts: Counter object with word frequencies\n",
    "        min_freq: Minimum frequency required to include a token\n",
    "        specials: List of special tokens to add\n",
    "        \n",
    "    Returns:\n",
    "        word_to_idx: Dictionary mapping tokens to indices\n",
    "        idx_to_word: Dictionary mapping indices to tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # Start with special tokens\n",
    "    word_to_idx: dict[str, int] = {token: idx for idx, token in enumerate(specials)}\n",
    "    # Sort word tokens by frequency (descending) \n",
    "    token_counts_sorted: list[tuple[str, int]] = sorted(token_counts.items(), \n",
    "                                                        key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Add tokens that meet minimum frequency\n",
    "    for word, count in token_counts_sorted:\n",
    "        if count >= min_freq and word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)  # new index starts with 2.\n",
    "    \n",
    "    # Create reverse mapping\n",
    "    idx_to_word: dict[int, str] = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "word_to_idx, idx_to_word = build_vocabulary(\n",
    "    token_counts, \n",
    "    min_freq=1, \n",
    "    specials=['<pad>', '<unk>']\n",
    "    )\n",
    "print(f\"Vocabulary size: {len(word_to_idx)}\")\n",
    "print([word_to_idx[word] for word in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be89ad",
   "metadata": {},
   "source": [
    "<b>Best practice</b><br>\n",
    "<br>\n",
    "Using special tokens like \"pad\" and \"unk\" in RNNs is a common practice for handling variable-length sequences and out-of-vocabulary words. Here are some best practices for their usage:\n",
    "\n",
    "- Use \"pad\" tokens to pad sequences to a fixed length. This ensures that all input sequences have the same length, which is necessary for efficient batch processing in neural networks. Pad sequences at the end rather than the beginning to preserve the order of the input data. When tokenizing text data, assign a unique integer index to the \"pad\" tokens and ensure that it corresponds to a vector of zeros in the embedding matrix.\n",
    "- Use \"unk\" tokens to represent out-of-vocabulary words that are not present in the vocabulary of the model. During inference, replace any words that are not present in the vocabulary with the \"unk\" tokens to ensure that the model can process the input.\n",
    "- Exclude \"pad\" tokens from contributing to the loss during training to avoid skewing the learning process.\n",
    "- Monitor the distribution of \"unk\" tokens in the dataset to assess the prevalence of out-of-vocabulary words and adjust the vocabulary size accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7077f2c",
   "metadata": {},
   "source": [
    "#### Create custom Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a3a301e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   10,   138,    29,  ...,     0,     0,     0],\n",
      "        [ 4690,  3821,     7,  ...,     2, 18444,  1046],\n",
      "        [   11,    18, 37583,  ...,     0,     0,     0],\n",
      "        [   11,  1158,  3396,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([319, 352, 170, 221], device='cuda:0')\n",
      "torch.Size([4, 352])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "# Create a custom dataset class for the movie reviews\n",
    "class MovieReviewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for movie reviews\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, word_to_idx: dict[str, int]) -> None:\n",
    "        self.data: pd.DataFrame = data\n",
    "        self.word_to_idx: dict[str, int] = word_to_idx\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        review: str = self.data.iloc[idx]['review']\n",
    "        sentiment: int = self.data.iloc[idx]['sentiment']\n",
    "        \n",
    "        # Tokenize and convert to indices\n",
    "        tokens: list[str] = tokenize(review)\n",
    "        indices: list[int] = [self.word_to_idx.get(token, self.word_to_idx['<unk>']) for token in tokens]\n",
    "        length: int = len(indices)\n",
    "        \n",
    "        return torch.tensor(indices), torch.tensor(sentiment), torch.tensor(length)\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a custom collate function to handle variable-length sequences\n",
    "def collate_fn(batch: list[tuple[Tensor, Tensor, Tensor]]) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Custom collate function to pad sequences and create batches\n",
    "    \"\"\"\n",
    "    # Unzip the batch into separate lists\n",
    "    reviews, sentiments, lengths = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    padded_reviews: Tensor = torch.nn.utils.rnn.pad_sequence(reviews, # reviews is a list of tensors\n",
    "                                                             batch_first=True, \n",
    "                                                             padding_value=word_to_idx['<pad>'])\n",
    "    # stack sentiments into a single tensor\n",
    "    sentiments: Tensor = torch.stack(sentiments)\n",
    "    # stack lengths into a single tensor\n",
    "    lengths: Tensor = torch.stack(lengths)\n",
    "    \n",
    "    return padded_reviews.to(device), sentiments.to(device), lengths.to(device)\n",
    "\n",
    "\n",
    "# Create DataLoader for training set\n",
    "torch.manual_seed(42) \n",
    "batch_size: int = 4\n",
    "train_dataset: MovieReviewDataset = MovieReviewDataset(train_data, word_to_idx)\n",
    "train_loader: DataLoader = DataLoader(train_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      shuffle=True, \n",
    "                                      collate_fn=collate_fn)\n",
    "\n",
    "# Check the first batch\n",
    "reviews, sentiments, lengths = next(iter(train_loader))\n",
    "print(reviews)\n",
    "print(sentiments)\n",
    "print(lengths)\n",
    "print(reviews.shape)  # (batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d498bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the DataSet and DataLoader for training, validation, and test sets\n",
    "batch_size: int = 32\n",
    "train_loader: DataLoader = DataLoader(train_dataset,\n",
    "                                      batch_size=batch_size, \n",
    "                                      shuffle=True, \n",
    "                                      collate_fn=collate_fn)\n",
    "val_dataset: MovieReviewDataset = MovieReviewDataset(val_data, word_to_idx)\n",
    "val_loader: DataLoader = DataLoader(val_dataset, \n",
    "                                     batch_size=batch_size, \n",
    "                                     shuffle=False, \n",
    "                                     collate_fn=collate_fn)\n",
    "test_dataset: MovieReviewDataset = MovieReviewDataset(test_data, word_to_idx)\n",
    "test_loader: DataLoader = DataLoader(test_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      shuffle=False, \n",
    "                                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c4a11",
   "metadata": {},
   "source": [
    "#### Build a LSTM network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e11524a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "# 1, Define the network hyperparameters\n",
    "vocab_size: int = len(word_to_idx)\n",
    "embedding_dim: int = 128\n",
    "hidden_dim: int = 64\n",
    "fc_hidden_dim: int = 32\n",
    "\n",
    "# 2, Define the model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple RNN model for sentiment analysis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, fc_hidden_dim: int) -> None:\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(    # convert input word indices into dense word embeddings.\n",
    "            vocab_size, \n",
    "            embedding_dim,\n",
    "            padding_idx=word_to_idx['<pad>']) # pad_idx=0 indicates padding tokens will be ignored during embedding.\n",
    "        self.rnn = nn.LSTM(input_size=embedding_dim, \n",
    "                           hidden_size=hidden_dim, \n",
    "                           num_layers=2,   # use stacked LSTM layers\n",
    "                           dropout=0.4,  # dropout between LSTM layers\n",
    "                           batch_first=True)  # means that the input has a batch size as the first dimension.\n",
    "        \n",
    "        # add a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(hidden_dim, fc_hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # The output layer is a single neuron with sigmoid activation for binary classification.\n",
    "        self.fc2 = nn.Linear(fc_hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Pack the sequences because the LSTM layer requires packed sequences.\n",
    "        # The packed sequence is a more efficient representation of the input data.\n",
    "        packed_x: PackedSequence = nn.utils.rnn.pack_padded_sequence(x, \n",
    "                                                                     lengths.cpu().numpy(), # should be a 1D CPU int64 tensor\n",
    "                                                                     batch_first=True, \n",
    "                                                                     enforce_sorted=False)\n",
    "        # RNN layer\n",
    "        packed_output, _ = self.rnn(packed_x)\n",
    "        \n",
    "        # Unpack the sequences because the LSTM layer returns a packed sequence.\n",
    "        # the unpacked output is a tensor of shape (batch_size, max_length, hidden_dim).\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, \n",
    "                                                     batch_first=True)\n",
    "        \n",
    "        # Get the last hidden state\n",
    "        # This final hidden state vector represents the LSTM's \"understanding\" of the entire sequence and \n",
    "        # serves as the feature vector for subsequent classification layers.\n",
    "        last_hidden_state: Tensor = output[torch.arange(output.size(0)), lengths - 1]\n",
    "        \n",
    "        # Apply dropout to the last hidden state\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(last_hidden_state))\n",
    "        x = self.fc2(x)\n",
    "        # Sigmoid activation\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x.squeeze()  # Remove the extra dimension\n",
    "    \n",
    "# 3, Initialize the model\n",
    "model: SentimentLSTM = SentimentLSTM(vocab_size, embedding_dim, \n",
    "                                     hidden_dim, fc_hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0cb5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4, Define the loss function and optimizer\n",
    "loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=0.001, weight_decay=0.01)  # AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1cf6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5, Define a function to evaluate the model\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch in test_loader:\n",
    "            reviews, sentiments, lengths = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(reviews, lengths)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, sentiments.float())\n",
    "            \n",
    "            # Calculate the total loss for the batch\n",
    "            total_loss += loss.item() * sentiments.size(0)\n",
    "            # Calculate accuracy\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            correct = (preds == sentiments.float()).float().sum().item()\n",
    "            total_accuracy += correct\n",
    "\n",
    "    # Compute average loss and accuracy for the test set\n",
    "    avg_loss: float = total_loss / len(test_loader.dataset)\n",
    "    avg_accuracy: float = total_accuracy / len(test_loader.dataset)\n",
    "    \n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "\n",
    "# 6, Define a function to train the model\n",
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader, \n",
    "                loss_fn: nn.Module, \n",
    "                optimizer: torch.optim.Optimizer, \n",
    "                num_epochs: int) -> None:\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    \n",
    "    best_loss = float('inf') # Initialize best loss to infinity for early stopping\n",
    "    patience = 4  # Number of epochs with no improvement after which training will be stopped\n",
    "    counter = 0  # Counter for early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        for batch in train_loader:\n",
    "            reviews, sentiments, lengths = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(reviews, lengths)  # Get the output from the model\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, sentiments.float())\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate the total loss for the batch\n",
    "            total_loss += loss.item() * sentiments.size(0)  # Multiply by batch size to get the total loss for the batch\n",
    "            # Calculate accuracy\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            correct = (preds == sentiments.float()).float().sum().item()\n",
    "            total_accuracy += correct  # Sum the correct predictions for the batch\n",
    "\n",
    "        # Compute average loss and accuracy for the epoch\n",
    "        avg_loss_train: float = total_loss / len(train_loader.dataset)\n",
    "        avg_accu_train: float = total_accuracy / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluate the model on the validation set for each epoch\n",
    "        avg_loss_val, avg_accu_val = evaluate_model(model, val_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training loss: {avg_loss_train:.4f}, Training accu: {avg_accu_train * 100:.2f}%,\" \n",
    "              f\" Val loss: {avg_loss_val:.4f}, Val accu: {avg_accu_val * 100:.2f}%\")\n",
    "\n",
    "        # Apply early stopping if the loss does not improve for 3 epochs on the validation set\n",
    "        if avg_loss_val < best_loss:\n",
    "            best_loss: float = avg_loss_val\n",
    "            # Save the model if it improves\n",
    "            torch.save(model.state_dict(), 'best_SentimentLSTM.pth')\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fc2258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training loss: 0.5831, Training accu: 68.67%, Val loss: 0.5144, Val accu: 74.94%\n",
      "Epoch [2/20], Training loss: 0.4273, Training accu: 81.22%, Val loss: 0.4117, Val accu: 81.02%\n",
      "Epoch [3/20], Training loss: 0.3068, Training accu: 87.89%, Val loss: 0.3027, Val accu: 87.62%\n",
      "Epoch [4/20], Training loss: 0.2041, Training accu: 92.55%, Val loss: 0.2916, Val accu: 88.72%\n",
      "Epoch [5/20], Training loss: 0.1396, Training accu: 95.31%, Val loss: 0.3201, Val accu: 88.10%\n",
      "Epoch [6/20], Training loss: 0.0967, Training accu: 96.86%, Val loss: 0.5235, Val accu: 84.32%\n",
      "Epoch [7/20], Training loss: 0.0703, Training accu: 97.87%, Val loss: 0.3884, Val accu: 89.22%\n",
      "Epoch [8/20], Training loss: 0.0486, Training accu: 98.62%, Val loss: 0.4086, Val accu: 88.96%\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# 7, Train the model\n",
    "num_epochs: int = 20\n",
    "train_model(model, train_loader, loss_fn, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7fd33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2845, Test Accuracy: 88.80%\n"
     ]
    }
   ],
   "source": [
    "# 8, Finally, evaluate the performance on the test set:\n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load('best_SentimentLSTM.pth', weights_only=True))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
